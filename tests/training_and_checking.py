#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sun Aug 15 12:22:16 2021
@author: aleix
"""

"""
Here we test the generative performance of the normalizing flow.
We will train a normalizing flow with data from an hmm model.
After training, the normalizing flow should be able to generate samples 
distributed as the hmm.
"""

import matplotlib.pyplot as plt
import torch
import numpy as np

from context import esn, flows


def compute_validation_loss(nf, esn_model, folder2load, n_val_batches):
    loss = 0
    for i in range(n_val_batches):
        batch_filename = "".join([folder2load, 'val', str(i), '.npy'])
        batch = torch.from_numpy(np.load(batch_filename)).float()
        
        batch_loss = validate(nf, esn_model, batch)
        loss += batch_loss
        
    loss /= n_val_batches
    return loss
    

def validate(nf, esn_model, sequences_batch):
    with torch.no_grad():
        loglike = nf.loglike_sequence(sequences_batch, esn_model)
        loss = -torch.mean(loglike)
    return loss.item()
        

def train(nf, esn_model, optimizer, sequences_batch):
    loglike = nf.loglike_sequence(sequences_batch, esn_model)
    loss = -torch.mean(loglike)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    return loss.item()

        
frame_dim = 1
seq_length = 10
n_train_batches = 1000
n_val_batches = n_train_batches // 5

hidden_dim = 8
folder2load = 'hmm_batches/'

esn_dim = 1000
hidden_dim = 16
num_flow_layers = 4
use_toeplitz = False
learning_rate = 1e-2

num_epochs = 10
num_gibbs_sampling = 50_000

nf = flows.NormalizingFlow(frame_dim, hidden_dim, 
                           num_flow_layers=num_flow_layers, esn_dim=esn_dim,
                           toeplitz=use_toeplitz)

n_model_params = pytorch_total_params = sum(
        p.numel() for p in nf.parameters() if p.requires_grad)
print("number of training batches", n_train_batches)
print("number of model parameters", n_model_params)


echo_state = esn.EchoStateNetwork(frame_dim, esn_dim=esn_dim)
optimizer = torch.optim.SGD(nf.parameters(), lr=learning_rate)

nf.train()

train_loss_evol = []
val_loss_evol = []

for epoch in range(num_epochs):
    print("epoch:", epoch)
    train_loss = 0
    for i in range(n_train_batches):
        train_batch_filename = "".join([folder2load, 'train', str(i), '.npy'])                                
        train_batch = torch.from_numpy(np.load(train_batch_filename)).float()
        
        train_batch_loss = train(nf, echo_state, optimizer, train_batch)
        train_loss += train_batch_loss
        
    train_loss /= n_train_batches
    train_loss_evol.append(train_loss)
    print("train loss:", train_loss)
    
    val_loss = compute_validation_loss(nf, echo_state, folder2load, n_val_batches)
    print("val loss:", val_loss, "\n")
    
    val_loss_evol.append(val_loss)
    # we stop training without patience if we overfit 
    if epoch > 0 and (val_loss_evol[-1] > val_loss_evol[-2]):
        break
        
plt.figure()
plt.plot(train_loss_evol, label="train")
plt.plot(val_loss_evol, label="validation")
plt.legend()

np.set_printoptions(precision=3)
# here we compare the theoretical expected value of the frames with the mean of 
# samples generated by the model

theoretical_expected = np.load(folder2load + 'expected_value_frames.npy')
print("theoretical expected for last frame")
print(theoretical_expected[-1])

nf_samples = nf.sample(seq_length, num_gibbs_sampling, echo_state)
frames_mean = torch.mean(nf_samples, dim=1)
print("model samples mean for last frame")
print(frames_mean[-1])

rel_diff = 100 * np.abs((frames_mean.numpy()[-1]-theoretical_expected[-1]) / (theoretical_expected[-1]))

print("relative difference (%)")

print(rel_diff)
