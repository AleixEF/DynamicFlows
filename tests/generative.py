#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sun Aug 15 12:22:16 2021

@author: aleix
"""

"""
Here we test the generative performance of the normalizing flow.
We will train a normalizing flow with data from an hmm model.
After training, the normalizing flow should be able to generate samples 
distributed as the hmm.
"""

import matplotlib.pyplot as plt
import torch
import numpy as np

from context import esn, flows


def train(nf, esn_model, optimizer, sequences_batch):
    loglike = nf.loglike_sequence(sequences_batch, esn_model)
    loss = -torch.mean(loglike)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    return loss.item()


def compute_frame_expected_value(frame_instant, nf, esn_object, 
                                 num_gibbs_sampling=10_000):
    with torch.no_grad():
        expected_value = torch.zeros(nf.frame_dim)
        for i in range(num_gibbs_sampling):
            # we sample frames until the frame instant we want
            sequence = nf.sample(seq_length=frame_instant+1, batch_size=1, 
                              esn_object=esn_object)
            frame = sequence[-1, 0, :]  # we are interested in the last frame
            expected_value += (frame / num_gibbs_sampling)
    return expected_value
        

frame_dim = 2
seq_length = 10
n_batches = 100
folder2load = "".join(['hmm_batches/', 'amount', str(n_batches), '/']) 

hidden_dim = 16
num_flow_layers = 4
learning_rate = 1e-3

nf = flows.NormalizingFlow(frame_dim, hidden_dim, num_flow_layers=num_flow_layers)
echo_state = esn.EchoStateNetwork(frame_dim)
optimizer = torch.optim.SGD(nf.parameters(), lr=learning_rate)

num_epochs = 30

nf.train()
loss_evol = []

for epoch in range(num_epochs):
    print("epoch:", epoch)
    dataset_loss = 0
    dataset_mean = 0
    for i in range(n_batches):
        batch_filename = "".join([folder2load, '64_', str(i), '.npy'])                                
        batch = torch.from_numpy(np.load(batch_filename)).float()
        
        batch_loss = train(nf, echo_state, optimizer, batch)
        dataset_loss += batch_loss
        
    dataset_loss /= n_batches
    loss_evol.append(dataset_loss)
    print("dataset loss:", dataset_loss, "\n")
    
plt.figure()
plt.plot(loss_evol)


# here we compare the theoretical expected value of a frame with the mean of 
# samples generated by the model
frame_instant = 0
theoretical_expected = np.load(folder2load + 'expected_value_frame' + str(frame_instant)+'.npy')

print("theoretical expected", theoretical_expected)

nf_expected = compute_frame_expected_value(frame_instant, nf, echo_state)
print("model samples mean:", nf_expected)



# plotting and comparing the emissions
points2plot = 64
nf_seq = nf.sample(frame_instant+1, points2plot, echo_state)
nf_frame = nf_seq[frame_instant]

hmm_frame = batch[frame_instant]

plt.figure()
plt.title("frame num %d" % frame_instant)
plt.plot(hmm_frame[:, 0], hmm_frame[:, 1], 
         "ro", markersize=3, label="hmm samples")
plt.plot(nf_frame[:, 0], nf_frame[:, 1], 
         "bo", markersize=3, label="nf samples")
plt.legend()

plt.show()



