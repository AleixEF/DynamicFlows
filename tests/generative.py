#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sun Aug 15 12:22:16 2021

@author: aleix
"""

"""
Here we test the generative performance of the normalizing flow.
We will train a normalizing flow with data from an hmm model.
After training, the normalizing flow should be able to generate samples 
distributed as the hmm.
"""

import matplotlib.pyplot as plt
import torch
import numpy as np

from context import esn, flows


def train(nf, esn_model, optimizer, sequences_batch):
    loglike = nf.loglike_sequence(sequences_batch, esn_model)
    loss = -torch.mean(loglike)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    return loss.item()


def compute_frame_expected_value(frame_instant, nf, esn_object, 
                                 num_gibbs_sampling=10_000):
    with torch.no_grad():
        expected_value = torch.zeros(nf.frame_dim)
        for i in range(num_gibbs_sampling):
            # we sample frames until the frame instant we want
            sequence = nf.sample(seq_length=frame_instant+1, batch_size=1, 
                              esn_object=esn_object)
            frame = sequence[-1, 0, :]  # we are interested in the last frame
            expected_value += (frame / num_gibbs_sampling)
    return expected_value
        

frame_dim = 2
seq_length = 10
batch_size = 128

hidden_dim = 64
num_flow_layers = 4
learning_rate = 1e-3

nf = flows.NormalizingFlow(frame_dim, hidden_dim, num_flow_layers=num_flow_layers)
echo_state = esn.EchoStateNetwork(frame_dim)
optimizer = torch.optim.SGD(nf.parameters(), lr=learning_rate)

# dataset loading 
dataset = torch.from_numpy(np.load('hmm_10000.npy')).float()

num_epochs = 40
num_dataset_batches = dataset.shape[1] // batch_size

nf.train()
loss_evol = []

for epoch in range(num_epochs):
    print("epoch:", epoch)
    dataset_loss = 0
    idx_start = 0
    idx_end = batch_size
    for _ in range(num_dataset_batches):
        batch = dataset[:, idx_start:idx_end, :]
        
        batch_loss = train(nf, echo_state, optimizer, batch)
        dataset_loss += batch_loss
        
        idx_start += batch_size
        idx_end += batch_size
        
    dataset_loss /= num_dataset_batches
    loss_evol.append(dataset_loss)
    print("dataset loss:", dataset_loss, "\n")
    
plt.figure()
plt.plot(loss_evol)

nf.eval()

# here we compare the theoretical expected value of a frame with the mean
# of the dataset and the mean of samples generated by the model
frame_instant = 0
theoretical_expected = [-3.07973318, -3.02017382]
print("theoretical expected", theoretical_expected)
print("dataset mean:", torch.mean(dataset[frame_instant], dim=0))

nf_expected = compute_frame_expected_value(frame_instant, nf, echo_state)
print("model samples mean:", nf_expected)


# plotting and comparing the emissions
nf_seq = nf.sample(frame_instant+1, batch_size, echo_state)

nf_frame = nf_seq[frame_instant]
hmm_frame = dataset[frame_instant, :batch_size, :]

plt.figure()
plt.title("frame num %d" % frame_instant)
plt.plot(hmm_frame[:, 0], hmm_frame[:, 1], 
         "ro", markersize=3, label="hmm samples")
plt.plot(nf_frame[:, 0], nf_frame[:, 1], 
         "bo", markersize=3, label="nf samples")
plt.legend()

plt.show()


